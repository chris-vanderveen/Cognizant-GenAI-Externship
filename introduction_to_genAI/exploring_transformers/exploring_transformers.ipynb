{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76f44b36-b846-435a-b618-2492dbfa1704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) temperature = 0.9\n",
      "\n",
      "Why our Ancestors Drilled Holes in Each Other's Skulls\" (SACRAMENTO)\n",
      "\n",
      "In the late 1990s, in response to the growing body of evidence linking paleontology research to the creation and spread of modern humans, the Grand Canyon National Park Conservancy took action by requiring paleontologists to examine their skulls and skulls to determine whether they represent human ancestors. For those looking to see if the skulls have been reconstructed by modern humans, paleoclimates were made to be placed in an oak or oak grove and placed into a pit dug in order to study the evolution of humans. They were then turned over to researchers who then examined them and had an opportunity to test the skull quality and analysis using an X-ray technique. Some of the skulls were altered in the process of development, such as those of African American, Asian, and possibly white (SACRAMENTO, p. 35). These skulls then were analyzed using a series of x-ray tomography scans (Klebs and Bones, 2002). The results showed that the DNA of those individuals showed a strong correlation with ancestry. Some of the bones of Native American Americans in the areas affected by the hominid, like the Upper Miowa area, were later found to be much older than their ancestral ancestors.\n",
      "\n",
      "The\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) temperature = 0.5\n",
      "\n",
      "Why our Ancestors Drilled Holes in Each Other's Skulls\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\"\n",
      "\n",
      "\"The New York Times\n",
      "\n",
      "(3) temperature = 0.1\n",
      "\n",
      "Why our Ancestors Drilled Holes in Each Other's Skulls\"\n",
      "\n",
      "\"The Ancestors Drilled Holes in Each Other's Skulls\"\n",
      "\n",
      "\"The Ancestors Drilled Holes in Each Other's Skulls\"\n",
      "\n",
      "\"The Ancestors Drilled Holes in Each Other's Skulls\"\n",
      "\n",
      "\"The Ancestors Drilled Holes in Each Other's Skulls\"\n",
      "\n",
      "\"The Ancestors Drilled Holes in Each Other's Skulls\"\n",
      "\n",
      "\"The Ancestors Drilled Holes in Each Other's Skulls\"\n",
      "\n",
      "\"The Ancestors Drilled Holes in Each Other's Skulls\"\n",
      "\n",
      "\"The Ancestors Drilled Holes in Each Other's Skulls\"\n",
      "\n",
      "\"The Ancestors Drilled Holes in Each Other's Skulls\"\n",
      "\n",
      "\"The Ancestors Drilled Holes in Each Other's Skulls\"\n",
      "\n",
      "\"The Ancestors Drilled Holes in Each Other's Skulls\"\n",
      "\n",
      "\"The Ancestors Drilled Holes in Each Other's Skulls\"\n",
      "\n",
      "\"The Ancestors Drilled Holes in Each Other's Skulls\"\n",
      "\n",
      "\"The Ancestors Drilled Holes in Each Other's Skulls\"\n",
      "\n",
      "\"The Ancestors Drilled Holes in Each Other's Skulls\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# News Headline prompt\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "prompt = \"Why our Ancestors Drilled Holes in Each Other's Skulls\"\n",
    "\n",
    "# Generate text with varying temperature settings for comparison\n",
    "result = generator(prompt, max_length=50, temperature=0.9, truncation=True)\n",
    "print(f\"(1) temperature = 0.9\\n\\n{result[0]['generated_text']}\\n\")\n",
    "\n",
    "result = generator(prompt, max_length=40, temperature=0.5, truncation=True)\n",
    "print(f\"(2) temperature = 0.5\\n\\n{result[0]['generated_text']}\\n\")\n",
    "\n",
    "result = generator(prompt, max_length=30, temperature=0.1, truncation=True)\n",
    "print(f\"(3) temperature = 0.1\\n\\n{result[0]['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb88c4c4-704e-48fe-9a96-2ad49b905a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) temperature = 0.9\n",
      "\n",
      "The air turned black all around me but no one else.\"\n",
      "\n",
      "\n",
      "I was standing at a foot-high fountain on a high-speed sidewalk, then the water turned black again and it was time to swim outside.\n",
      "\n",
      "\"Just go to the beach. Go to the back,\" said Mark to me. \"It will be okay.\"\n",
      "\n",
      "I ran up to a small plastic tube which held what appeared to be an enormous swimming pool. The water was thick with water and I knew I was in for another big swim.\n",
      "\n",
      "\n",
      "\"It's big,\" said Mark to me, with a smile that seemed to stretch away from his face. \"It's big.\"\n",
      "\n",
      "The water was a little heavy but Mark was able to move the swimmers just a couple inches above him, and even with only one swimmer left he managed to swim five inches underwater. He was able to reach 5 feet at least.\n",
      "\n",
      "\n",
      "Mark had a big grin on his face that was reminiscent of someone who has a big smile.\n",
      "\n",
      "\"It was fun to swim around with.\"\n",
      "\n",
      "\n",
      "The people who were coming to the beach to see me were all very happy, although I had to admit it was a bit nerve-wracking because it felt like a trip from my hometown to another city.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) temperature = 0.5\n",
      "\n",
      "The air turned black all around me. The sun was setting. The air was hot and the cold. It was cold, and it was cold like a cold day. I was on the ground. I was sitting on the ground, and my eyes were closed. My whole body was cold. My whole body was cold. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not happy. I was not\n",
      "\n",
      "(3) temperature = 0.1\n",
      "\n",
      "The air turned black all around me. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock. I was in a state of shock\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Short story opener prompt\n",
    "prompt = 'The air turned black all around me'\n",
    "\n",
    "# Generate text with varying temperature settings for comparison\n",
    "result = generator(prompt, max_length=50, temperature=0.9, truncation=True)\n",
    "print(f\"(1) temperature = 0.9\\n\\n{result[0]['generated_text']}\\n\")\n",
    "\n",
    "result = generator(prompt, max_length=40, temperature=0.5, truncation=True)\n",
    "print(f\"(2) temperature = 0.5\\n\\n{result[0]['generated_text']}\\n\")\n",
    "\n",
    "result = generator(prompt, max_length=30, temperature=0.1, truncation=True)\n",
    "print(f\"(3) temperature = 0.1\\n\\n{result[0]['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8ccc828-4ce2-46f5-9c19-3245a17522d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) temperature = 0.9\n",
      "\n",
      "\"I'm not buying that umbrella,\" Sophie said indignantly, \"It's too expensive!\"\n",
      "\n",
      "\"You know what? I thought it was going to be over soon,\" she continued, \"I thought about you, Sophie. I know you're not giving me anything to lose, but it's my fault for not staying.\"\n",
      "\n",
      "Sophie looked around the room, realizing that she had become something of a burden. \"Well, I still feel terrible about this girl,\" she said. \"You're so good with your hands, Sophie. I'm glad you brought a bottle with you! And I still have this little basket of water!\"\n",
      "\n",
      "\"We haven't done much together,\" the woman said. \"I thought we could go through it without you, but I don't think you can.\"\n",
      "\n",
      "\"You don't mind,\" Sophie said. \"The whole time, I've been working on getting your help. I just have to get you what you want. And my sister is a good one. She'll get you where you always have to be, right?\"\n",
      "\n",
      "\"She's not gonna be able to find enough help. And so, I don't think we'll ever talk about it with each other,\" the woman said. \"It's not like I have anything much to say about it.\"\n",
      "\n",
      "\"I'm\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) temperature = 0.5\n",
      "\n",
      "\"I'm not buying that umbrella,\" Sophie said indignantly, \"It's too expensive!\"\n",
      "\n",
      "\"I'm not buying that umbrella,\" I said indignantly, \"It's too expensive!\"\n",
      "\n",
      "\"I don't know what you're talking about, Sophie,\" she said, \"I'm not buying that umbrella.\"\n",
      "\n",
      "\"It's too expensive!\" I said indignantly, \"I'm not buying that umbrella.\"\n",
      "\n",
      "\"I don't know what you're talking about,\" Sophie said indignantly, \"I'm not buying that umbrella.\"\n",
      "\n",
      "\"I don't know what you're talking about, Sophie,\" I said indignantly, \"I'm not buying that umbrella.\"\n",
      "\n",
      "\"I don't know what you're talking about, Sophie,\" I said indignantly, \"I'm not buying that umbrella.\"\n",
      "\n",
      "\"I don't know what you're talking about, Sophie,\" I said indignantly, \"I'm not buying that umbrella.\"\n",
      "\n",
      "\"I don't know what you're talking about, Sophie,\" I said indignantly, \"I'm not buying that umbrella.\"\n",
      "\n",
      "\"I don't know what you're talking about, Sophie,\" I said indignantly, \"I'm not buying that umbrella.\"\n",
      "\n",
      "\"I don't know what you're talking about, Sophie,\" I said indignantly, \"I\n",
      "\n",
      "(3) temperature = 0.1\n",
      "\n",
      "\"I'm not buying that umbrella,\" Sophie said indignantly, \"It's too expensive!\"\n",
      "\n",
      "\"I'm not buying that umbrella,\" Sophie said indignantly, \"It's too expensive!\"\n",
      "\n",
      "\"I'm not buying that umbrella,\" Sophie said indignantly, \"It's too expensive!\"\n",
      "\n",
      "\"I'm not buying that umbrella,\" Sophie said indignantly, \"It's too expensive!\"\n",
      "\n",
      "\"I'm not buying that umbrella,\" Sophie said indignantly, \"It's too expensive!\"\n",
      "\n",
      "\"I'm not buying that umbrella,\" Sophie said indignantly, \"It's too expensive!\"\n",
      "\n",
      "\"I'm not buying that umbrella,\" Sophie said indignantly, \"It's too expensive!\"\n",
      "\n",
      "\"I'm not buying that umbrella,\" Sophie said indignantly, \"It's too expensive!\"\n",
      "\n",
      "\"I'm not buying that umbrella,\" Sophie said indignantly, \"It's too expensive!\"\n",
      "\n",
      "\"I'm not buying that umbrella,\" Sophie said indignantly, \"It's too expensive!\"\n",
      "\n",
      "\"I'm not buying that umbrella,\" Sophie said indignantly, \"It's too expensive!\"\n",
      "\n",
      "\"I'm not buying that umbrella,\" Sophie said indignantly, \"It's too expensive!\"\n",
      "\n",
      "\"I'm not buying that umbrella,\" Sophie said indignantly, \"It's too expensive!\"\n",
      "\n",
      "\"I\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dialogue prompt example\n",
    "prompt = '\"I\\'m not buying that umbrella,\" Sophie said indignantly, \"It\\'s too expensive!\"'\n",
    "\n",
    "# Generate text with varying temperature settings for comparison\n",
    "result = generator(prompt, max_length=50, temperature=0.9, truncation=True)\n",
    "print(f\"(1) temperature = 0.9\\n\\n{result[0]['generated_text']}\\n\")\n",
    "\n",
    "result = generator(prompt, max_length=40, temperature=0.5, truncation=True)\n",
    "print(f\"(2) temperature = 0.5\\n\\n{result[0]['generated_text']}\\n\")\n",
    "\n",
    "result = generator(prompt, max_length=30, temperature=0.1, truncation=True)\n",
    "print(f\"(3) temperature = 0.1\\n\\n{result[0]['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3496284-a186-4c74-9284-0b7b4796cb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) temperature = 0.9\n",
      "\n",
      "As glaciers move towards the ocean, they can sometimes grind along the ground, which leads debris to get mixed up inside. (Photo: Courtesy of National Marine Fisheries Service)\n",
      "\n",
      "There has always been a large amount of debris in some places but not enough to break through the ice. That's why a lot of the debris can come out of the water and not escape in the first place.\n",
      "\n",
      "But since glacial erosion began almost 20,000 years ago — the largest ever recorded — it's been pretty steady – especially for a place like Australia.\n",
      "\n",
      "There are some major sinkholes that are still visible near Antarctic waters. The one near Bristol was dug in 1997, and the bottom of the hole is actually about 20 metres deep and about 30 metres wide.\n",
      "\n",
      "So these sinkholes that were dug in were actually actually about 10 metres deep, and they're not going to change over the next few decades.\n",
      "\n",
      "The first person to observe them in the area of Bristol in 1997 was Neil Pearsall, an Antarctic scientist based at the University of East Anglia.\n",
      "\n",
      "He saw the huge surface sinkholes and he said they were about 100 metres larger than the current surface sinkholes.\n",
      "\n",
      "\"Because these are so large, you can see these places around the coast … where in the past, there would have been an enormous amount of debris that would have traveled\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) temperature = 0.5\n",
      "\n",
      "As glaciers move towards the ocean, they can sometimes grind along the ground, which leads debris to get mixed up inside.\n",
      "\n",
      "\"We think that this is probably the most significant event in the history of the ice sheet,\" said Greg Hahn, a glaciologist at the University of Alberta in Edmonton.\n",
      "\n",
      "The ice sheet is about 1.5 million square kilometres (1.3 million square miles) thick, or about the size of New York City. It's the largest in the world.\n",
      "\n",
      "\"It's a pretty big chunk of ice, and it's about the size of a football field,\" Hahn said.\n",
      "\n",
      "The first ice sheets were about 1.5 million square kilometres (1.6 million square miles) thick in the late 19th century.\n",
      "\n",
      "\"That's the largest ice sheet ever observed. It's the largest in the world. The first ice sheets were about 1.5 million square kilometres (1.6 million square miles) thick in the late 19th century,\" Hahn said.\n",
      "\n",
      "The ice sheet is about 1.5 million square kilometres (1.6 million square miles) thick, or about the size of New York City. (CBC)\n",
      "\n",
      "\"That's about the size of New York City. It's one of the largest ice sheets ever observed,\" he said.\n",
      "\n",
      "\"It's a pretty big\n",
      "\n",
      "(3) temperature = 0.1\n",
      "\n",
      "As glaciers move towards the ocean, they can sometimes grind along the ground, which leads debris to get mixed up inside.\n",
      "\n",
      "\"The ice is very dense, so it's hard to get a good picture of the ice,\" said Dr. David K. Karp, a glaciologist at the University of California, Berkeley.\n",
      "\n",
      "\"We're seeing a lot of debris, so it's hard to tell what's going on,\" he said.\n",
      "\n",
      "The researchers found that the ice is not melting at all.\n",
      "\n",
      "\"It's not melting at all,\" Karp said. \"It's not melting at all. It's not melting at all.\"\n",
      "\n",
      "The researchers found that the ice is not melting at all.\n",
      "\n",
      "\"It's not melting at all,\" Karp said. \"It's not melting at all.\"\n",
      "\n",
      "The researchers found that the ice is not melting at all.\n",
      "\n",
      "\"It's not melting at all,\" Karp said. \"It's not melting at all.\"\n",
      "\n",
      "The researchers found that the ice is not melting at all.\n",
      "\n",
      "\"It's not melting at all,\" Karp said. \"It's not melting at all.\"\n",
      "\n",
      "The researchers found that the ice is not melting at all.\n",
      "\n",
      "\"It's not melting at all,\" Karp said. \"It's not melting at all.\"\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question or factual statement prompt example\n",
    "prompt = \"As glaciers move towards the ocean, they can sometimes grind along the ground, which leads debris to get mixed up inside.\"\n",
    "\n",
    "# Generate text with varying temperature settings for comparison\n",
    "result = generator(prompt, max_length=50, temperature=0.9, truncation=True)\n",
    "print(f\"(1) temperature = 0.9\\n\\n{result[0]['generated_text']}\\n\")\n",
    "\n",
    "result = generator(prompt, max_length=40, temperature=0.5, truncation=True)\n",
    "print(f\"(2) temperature = 0.5\\n\\n{result[0]['generated_text']}\\n\")\n",
    "\n",
    "result = generator(prompt, max_length=30, temperature=0.1, truncation=True)\n",
    "print(f\"(3) temperature = 0.1\\n\\n{result[0]['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d083331-909f-4021-a3f0-456e14a116d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-starter",
   "language": "python",
   "name": "conda-env-python-starter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
