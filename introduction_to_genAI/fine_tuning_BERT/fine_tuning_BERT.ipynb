{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e52c79-fe4a-48a0-8947-75c2e9829ab0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Fine Tuning BERT For Paraphrase Classification\n",
    "Fine tuning BERT for paraphrase classification using the Microsoft Research Paraphrase Classification dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8cebe6-8419-48af-b42a-3feba6d1133b",
   "metadata": {},
   "source": [
    "# Part 1: Fine-Tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8535ff9-3129-4d88-9085-0732a5c15da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA CUDA backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_73705/801247998.py:68: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1150' max='1150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1150/1150 28:28, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.478200</td>\n",
       "      <td>0.362089</td>\n",
       "      <td>0.838235</td>\n",
       "      <td>0.881720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.253700</td>\n",
       "      <td>0.490022</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.899329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.671470</td>\n",
       "      <td>0.860294</td>\n",
       "      <td>0.899115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>0.738737</td>\n",
       "      <td>0.855392</td>\n",
       "      <td>0.899145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.825924</td>\n",
       "      <td>0.865196</td>\n",
       "      <td>0.906303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "Saving the best model to ./final_mrpc_model\n",
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "\n",
    "# Check if MPS is available and set the device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple Silicon (MPS) backend.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using NVIDIA CUDA backend.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU backend.\")\n",
    "\n",
    "# Load the full dataset dictionary (train and validation splits)\n",
    "dataset_dict = load_dataset(\"nyu-mll/glue\", \"mrpc\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the dataset, truncate and pad text\n",
    "def encode(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(encode, batched=True)\n",
    "\n",
    "# Rename the label column to labels and remove unnecessary columns\n",
    "tokenized_datasets = tokenized_datasets.map(lambda examples: {\"labels\": examples[\"label\"]}, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\", \"label\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Get the separate train and validation datasets\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "# Define the metric computation function\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mrpc-results\",      # Directory to save the model and results\n",
    "    logging_dir='./mrpc-logs',        # Directory for logs\n",
    "    num_train_epochs=5,               # Total number of training epochs\n",
    "    per_device_train_batch_size=16,   # Batch size for training\n",
    "    per_device_eval_batch_size=16,    # Batch size for evaluation\n",
    "    \n",
    "    # --- Settings for Tracking Metrics ---\n",
    "    eval_strategy=\"epoch\",            # Run evaluation at the end of each epoch\n",
    "    logging_strategy=\"steps\",         # Log metrics during training\n",
    "    logging_steps=50,                 # Log training loss every 50 steps\n",
    "    \n",
    "    # --- Settings for Saving the Model ---\n",
    "    save_strategy=\"epoch\",            # Save a checkpoint at the end of each epoch\n",
    "    load_best_model_at_end=True,      # Load the best model found during training\n",
    "    metric_for_best_model=\"accuracy\", # Use accuracy to determine the best model\n",
    ")\n",
    "\n",
    "# 3. Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 4. Start Training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# 5. Save the final best model\n",
    "final_model_path = \"./final_mrpc_model\"\n",
    "print(f\"Saving the best model to {final_model_path}\")\n",
    "trainer.save_model(final_model_path)\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1b5e5c-0bbd-4a8e-8fd8-6a93eff0eec0",
   "metadata": {},
   "source": [
    "# Part 2: Debugging Issues\n",
    "\n",
    "I ran into some small issues with properly installing and importing some of the required modules to train the model. These were quite trivial to solve however.\n",
    "\n",
    "A larger issue that I encountered was with training time. Although I have a MacBook that is MPS compatible the training time for me was extremely slow and it was also causing issues with the performance of my machine outside of the training task. Fortunately I have a desktop with a CUDA enabled graphics card, so I was able to figure out how to ensure the training was completed using CUDA and my GPU which reduced my training time by 91.6%. This allowed me to complete more training epochs in a reasonable amount of time and reduce loss significantly from 11.3% to 1.5%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb16a68-75ed-4644-bab8-73e71ed738f3",
   "metadata": {},
   "source": [
    "# Part 3: Evaluating the Model\n",
    "\n",
    "We will evaluate the final model on the validation set provided with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d94375c-e751-4e7d-8c31-fc7720c6fa40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Metrics ---\n",
      "Accuracy: 0.8652\n",
      "F1-Score: 0.9063\n",
      "Loss: 0.8259\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"--- Evaluation Metrics ---\")\n",
    "print(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "print(f\"F1-Score: {eval_results['eval_f1']:.4f}\")\n",
    "print(f\"Loss: {eval_results['eval_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc5cac2-4374-4377-aa15-b64e53c21a2e",
   "metadata": {},
   "source": [
    "# Refining the Model\n",
    "\n",
    "Now that we have a baseline for model performance we can make refinements to our learning rate, weight decay, and add a warmup schedule and compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d03cc04-480a-4826-b5b5-6d1892b068d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Metrics ---\n",
      "Accuracy: 0.8603\n",
      "F1-Score: 0.8966\n",
      "Loss: 0.3824\n"
     ]
    }
   ],
   "source": [
    "# Output the eval for experimental refined model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"--- Evaluation Metrics ---\")\n",
    "print(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "print(f\"F1-Score: {eval_results['eval_f1']:.4f}\")\n",
    "print(f\"Loss: {eval_results['eval_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6e4b48-135f-471d-9e71-b290411a19c6",
   "metadata": {},
   "source": [
    "# Analysis of Refinement\n",
    "## 1. Initial Model Performance (Before Refinement)\n",
    "\n",
    "--- Evaluation Metrics ---\n",
    "- Accuracy: 0.8652\n",
    "- F1-Score: 0.9063\n",
    "- Loss: 0.8259\n",
    "\n",
    "Based on the initial results, we attempted to refine the model to improve generalization. I hypothesized that the default learning rate might be too high, causing the model to converge too quickly and potentially overfit. I made the following hyperparameter adjustments:\n",
    "- Reduced learning rate from `5e-5` to `2e-5`.\n",
    "- Added weight decay, set to `0.01` to add regularization.\n",
    "- Added warmup steps to introduce a period of smaller learning steps before training at defined level.\n",
    "\n",
    "## 2. Refined Model Performance\n",
    "\n",
    "--- Evaluation Metrics ---\n",
    "- Accuracy: 0.8603\n",
    "- F1-Score: 0.8966\n",
    "- Loss: 0.3824\n",
    "\n",
    "## 3. Compare and Analyze Results\n",
    "\n",
    "We can see that with our refinements the model has in fact become very overconfident. The log loss has decreased significantly with a more stable training process, however our accuracy and F1-Scores have both decreased slightly in comparison to the original, untuned hyperparameters. It seems that our model has learned the patterns in the training data far more specifically such that it doesn't generalize as well as the untuned model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a3723a-2dd7-4548-ae6a-a17dd5fa83b0",
   "metadata": {},
   "source": [
    "# Part 4: Creative Application\n",
    "\n",
    "For part 4 I have decided to train the DistilBERT model on a sentiment analysis problem using the Yelp Review full dataset. The goal is to classify reviews into one of five star ratings. I have chosen DistilBERT becuase it is a smaller version of BERT that retains performance well from the foundation model which will result in much faster training, reducing training time, and thereby increasing experimental iteration speeds. To optimize performance, I implemented mixed-precision training (fp16=True) to accelerate the process and an EarlyStoppingCallback to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a83382f1-340d-4a79-8611-f4fcfd90402e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA CUDA backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff083a0e4b249d7820949af5b97c1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bcbf5c39ce4927baefba119c80c3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73705/4140540532.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='121875' max='121875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [121875/121875 6:26:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.706000</td>\n",
       "      <td>0.719727</td>\n",
       "      <td>0.689120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.642600</td>\n",
       "      <td>0.699429</td>\n",
       "      <td>0.694660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.560700</td>\n",
       "      <td>0.726915</td>\n",
       "      <td>0.695500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "Saving the best model to ./final_yelp_model\n",
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Check if MPS is available and set the device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple Silicon (MPS) backend.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using NVIDIA CUDA backend.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU backend.\")\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# There are 5 stars so the number of labels is 5\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def encode(examples):\n",
    "    # dataset has a single 'text' field\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# tokenize the dataset\n",
    "tokenized_datasets = dataset.map(encode, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.map(lambda examples: {\"labels\": examples[\"label\"]}, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"label\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./yelp-distilbert-results\",\n",
    "    num_train_epochs=3, \n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True,  # Enable Mixed Precision Training\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,a\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # Stop if metric doesn't improve for 3 evaluations\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Save the final best model\n",
    "final_model_path = \"./final_yelp_model\"\n",
    "print(f\"Saving the best model to {final_model_path}\")\n",
    "trainer.save_model(final_model_path)\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "990cec12-50b4-4291-b282-b0b333b60bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA CUDA backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_73705/678575060.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='162500' max='162500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [162500/162500 8:23:47, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.729300</td>\n",
       "      <td>0.747476</td>\n",
       "      <td>0.678140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.674800</td>\n",
       "      <td>0.725310</td>\n",
       "      <td>0.686920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.582900</td>\n",
       "      <td>0.740100</td>\n",
       "      <td>0.687440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.479700</td>\n",
       "      <td>0.826032</td>\n",
       "      <td>0.683440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "Saving the best model to ./final_yelp_model\n",
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Check if MPS is available and set the device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple Silicon (MPS) backend.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using NVIDIA CUDA backend.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU backend.\")\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# There are 5 stars so the number of labels is 5\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def encode(examples):\n",
    "    # dataset has a single 'text' field\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# tokenize the dataset\n",
    "tokenized_datasets = dataset.map(encode, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.map(lambda examples: {\"labels\": examples[\"label\"]}, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"label\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./yelp-distilbert-results\",\n",
    "    num_train_epochs=4, \n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=250,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True,  # Enable Mixed Precision Training\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # Stop if metric doesn't improve for 3 evaluations\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Save the final best model\n",
    "final_model_path = \"./final_yelp_model\"\n",
    "print(f\"Saving the best model to {final_model_path}\")\n",
    "trainer.save_model(final_model_path)\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cabf6c7-529c-41a2-a7cb-9178c1678966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
